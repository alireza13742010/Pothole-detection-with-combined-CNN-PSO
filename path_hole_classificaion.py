# -*- coding: utf-8 -*-
"""Path hole classificaion.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ur5cDqqBszDHJmlDT0yHEMrZN3Pu6gSN
"""

from google.colab import drive
drive.mount('/content/drive')

!pip install pyunpack

!pip install patool

from pyunpack import Archive
Archive('/content/drive/MyDrive/path hole classifcation.7z').extractall("/content/drive/MyDrive")

import os
from PIL import Image, ImageDraw
import torchvision.transforms as transforms
import cv2
import numpy as np
import pandas as pd

Potholes_test =  os.listdir('/content/test')
Potholes_valid =  os.listdir('/content/valid')
Potholes_train =  os.listdir('/content/train')
Healthy =  os.listdir('/content/drive/MyDrive/Healthy road')

Potholes_test1=[]
IMAGE_WIDTH = 128
IMAGE_HEIGHT = 128
for x in Potholes_test:
  path = os.path.join('test',x)
  img = cv2.imread(path)
  img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
  Potholes_test1.append(img)
Potholes_test1 = np.array(Potholes_test1)
print(Potholes_test1.shape)

Potholes_train1=[]
IMAGE_WIDTH = 128
IMAGE_HEIGHT = 128
for x in Potholes_train:
  path = os.path.join('train',x)
  img = cv2.imread(path)
  img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
  Potholes_train1.append(img)
Potholes_train1 = np.array(Potholes_train1)
print(Potholes_train1.shape)

Potholes_valid1=[]
IMAGE_WIDTH = 128
IMAGE_HEIGHT = 128
for x in Potholes_valid:
  path = os.path.join('valid',x)
  img = cv2.imread(path)
  img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
  Potholes_valid1.append(img)
Potholes_valid1 = np.array(Potholes_valid1)
print(Potholes_valid1.shape)

Healthy_road1=[]
IMAGE_WIDTH = 128
IMAGE_HEIGHT = 128
for x in Healthy:
  path = os.path.join('/content/drive/MyDrive/Healthy road',x)
  img = cv2.imread(path)
  img = cv2.resize(img, (IMAGE_HEIGHT, IMAGE_WIDTH))
  Healthy_road1.append(img)
Healthy_road1 = np.array(Healthy_road1)
print(Healthy_road1.shape)

train = []
for x in Potholes_train1:
  train.append(x)
for x in Potholes_valid1:
  train.append(x)  
for x in Healthy_road1[0:int(0.8*Healthy_road1.shape[0])]:
  train.append(x)
train = np.array(train)  
print("Train zise is: ",train.shape)

test = []
for x in Potholes_test1:
  test.append(x)
for x in Healthy_road1[int(0.8*Healthy_road1.shape[0]):]:
  test.append(x)
test = np.array(test)  
print("Train zise is: ",test.shape)

target_train_pic = [1 for i in range(598)]
for x in [0 for i in range(77)]:
    target_train_pic.append(x)

target_test_pic = [1 for i in range(67)]
for x in [0 for i in range(20)]:
    target_test_pic.append(x) 
target_test_pic = np.array(target_test_pic) 
target_train_pic = np.array(target_train_pic)
train_pic = np.array(train)
test_pic = np.array(test)        
print(target_train_pic.shape)
print(train_pic.shape)    
print(target_test_pic.shape)
print(test_pic.shape)

from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(sparse=False)
target_train_pic2 = enc.fit_transform(target_train_pic.reshape(-1,1))
target_train_pic2 = np.array(target_train_pic2)
print(target_train_pic2.shape)

from tensorflow.keras.utils import to_categorical
from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(sparse=False)
target_test_pic2 = enc.fit_transform(target_test_pic.reshape(-1,1))
target_test_pic2 = np.array(target_test_pic2)
print(target_test_pic2.shape)

import tensorflow as tf
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm

from sklearn.model_selection import train_test_split, KFold
from tensorflow.keras.models import Model, load_model
from tensorflow.keras.layers import BatchNormalization, Conv2D, Activation,\
    MaxPooling2D, Conv1D,Conv2DTranspose, Dropout, Input, Concatenate, AveragePooling1D,Lambda, Average, Dense, Flatten,LSTM,GRU
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint,\
    ReduceLROnPlateau, LearningRateScheduler, CSVLogger
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.layers import Lambda, Conv2D, Activation,\
    BatchNormalization, UpSampling2D, multiply, add
from tensorflow.keras import backend as K

from keras.backend import sigmoid
def swish(x, beta = 1):
    return (x * sigmoid(beta * x))

from keras.utils.generic_utils import get_custom_objects
from keras.layers import Activation
get_custom_objects().update({'swish': Activation(swish)})

import tensorflow as tf
from tensorflow.keras.layers import Input,Dense,Conv2D,Add
from tensorflow.keras.layers import SeparableConv2D,ReLU
from tensorflow.keras.layers import BatchNormalization,MaxPool2D
from tensorflow.keras.layers import GlobalAvgPool2D
from tensorflow.keras import Model

from keras.utils.generic_utils import get_custom_objects
from keras.layers import Activation
get_custom_objects().update({'swish': Activation(swish)})

import tensorflow as tf
from tensorflow.keras.layers import Input,Dense,Conv2D,Add
from tensorflow.keras.layers import SeparableConv2D,ReLU
from tensorflow.keras.layers import BatchNormalization,MaxPool2D
from tensorflow.keras.layers import GlobalAvgPool2D
from tensorflow.keras import Model

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train, test, target_train_pic2, target_test_pic2
print('Train set shape is',X_train.shape)
print('Test set shape is',X_test.shape)
print("Target Train shape", y_train.shape)
print("Target Test shape", y_test.shape)

from keras.backend import sigmoid
def swish(x, beta = 1):
    return (x * sigmoid(beta * x))

from keras.utils.generic_utils import get_custom_objects
from keras.layers import Activation
get_custom_objects().update({'swish': Activation(swish)})

import keras
kernel_init = keras.initializers.glorot_uniform()
bias_init = keras.initializers.Constant(value=0.2)

from tensorflow.keras import callbacks
from tensorflow.keras.callbacks import EarlyStopping
import tensorflow as tf
from tensorflow import nn
from tensorflow import keras
from tensorflow.python.keras.layers import Input, Dense, Dropout,Conv2D,MaxPooling2D,Activation,Flatten,AveragePooling2D,GlobalMaxPooling2D,ZeroPadding2D, GlobalAveragePooling2D, MaxPool2D
from tensorflow.python.keras.models import Sequential
from tensorflow import keras
from keras import layers

from tensorflow.keras.models import *
from tensorflow.keras.layers import *
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import categorical_crossentropy
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.metrics import confusion_matrix
from tensorflow.keras.applications.densenet import DenseNet121
from tensorflow.keras.callbacks import *
from tensorflow.keras import backend as K
K.clear_session()

input1 = keras.layers.Input(shape=[128,128,3])
z = Conv2D(filters=64, kernel_size=3, activation='tanh',data_format='channels_last',padding='same',name= 'conv1d_20')(input1)
z = Conv2D(filters=64, kernel_size=3, activation='tanh',data_format='channels_last',padding='same')(z)
z = MaxPooling2D(pool_size=2,padding='same')(z)
z = Dropout(rate=0.25)(z)
z = Conv2D(filters=128, kernel_size=3, activation='tanh',data_format='channels_last',padding='same')(z)
z = Conv2D(filters=128, kernel_size=3, activation='tanh',data_format='channels_last',padding='same')(z)
z = MaxPooling2D(pool_size=2,padding='same')(z)
z = Flatten(name = 'flatten_5')(z)
z = Dense(64)(z)
z = Dropout(rate=0.20)(z)
z = Dense(32)(z)
z = Dropout(rate=0.20)(z)
out = Dense(2,activation='softmax')(z)
model1 = Model(inputs=input1,outputs= out)

import tensorflow as tf
early_stopping_cb =  tf.keras.callbacks.EarlyStopping(patience=75,
restore_best_weights=True)
lr_scheduler1 = tf.keras.callbacks.ReduceLROnPlateau(factor=0.333, patience=20)

model1.compile(optimizer='nadam', loss='categorical_crossentropy',metrics=['accuracy'])

history =model1.fit(x=X_train, y=y_train, batch_size=128,validation_batch_size=128, epochs=60, verbose=2,validation_data=(X_test, y_test),callbacks=[early_stopping_cb])

hist =history

import pandas as pd
import matplotlib.pyplot as plt
plt.rcParams["figure.figsize"] = (10,10)
plt.plot(hist.history['loss'],label ="Train_loss",linestyle='solid', linewidth = 2,marker='o', markersize=6)
plt.plot(hist.history['val_loss'],label ="Valid_loss",linestyle='--', linewidth = 2,marker='x', markersize=6)
plt.plot(hist.history['accuracy'],label ="Train_loss",linestyle='solid', linewidth = 2,marker='o', markersize=6)
plt.plot(hist.history['val_accuracy'],label ="Valid_loss",linestyle='--', linewidth = 2,marker='x', markersize=6)
plt.gca().set_xlim(0, 150)
plt.gca().set_ylim(0, 10) # set the vertical range to [0-1]
plt.xlabel('Number of Epochs', fontsize=20)
plt.ylabel('Loss & Accuracy', fontsize=20)
#plt.title('CNN model for PD detection',fontsize=16)
plt.legend()
plt.grid(True)
plt.show()

from tensorflow.keras.applications.resnet50 import ResNet50 
from tensorflow.keras import Model
from tensorflow.keras import Input
#input = Input(shape=(X_train.shape[-3],X_train.shape[-2],X_train.shape[-1]))
#conv_base = ResNet50(weights='imagenet',include_top=False,input_shape=(X_train.shape[-3],X_train.shape[-2],X_train.shape[-1]),input_tensor=input)
#ResNet_layer1 =  conv_base.get_layer('conv1_conv').output
#ResNet_layer2 =  conv_base.get_layer('conv2_block1_1_bn').output
#ResNet_layer10 =  conv_base.get_layer('conv2_block2_add').output
#ResNet_layer20 =  conv_base.get_layer('conv4_block3_add').output
#ResNet_layer40 =  conv_base.get_layer('conv5_block1_add').output
#ResNet_layer50 =  conv_base.get_layer('conv5_block3_out').output
#vgg_layer17 =  conv_base.get_layer('block5_pool').output



from tensorflow.keras import Model
from tensorflow.keras import Input
input = Input(shape=(128,128,3))
base_model_flatten_out_put =  model1.get_layer('flatten_5').output
base_model_first_conv_out_put =  model1.get_layer('conv1d_20').output

Flatten_Model = Model(inputs=input1,outputs= base_model_flatten_out_put)
First_Conv=_Model = Model(inputs=input1,outputs= base_model_first_conv_out_put)

out_flatten = Flatten_Model.predict(X_train)

out_flatten_test = Flatten_Model.predict(X_test)

print(out_flatten.shape)

from sklearn.decomposition import PCA
pca = PCA()
out_pca= pca.fit(out_flatten)
cumsum = np.cumsum(pca.explained_variance_ratio_)
d = np.argmax(cumsum >= 0.70) + 1
print(d)



pca = PCA(n_components=d)
train_flatten_out = pca.fit_transform(out_flatten)
test_flatten_out = pca.fit_transform(out_flatten_test)

!pip install pyswarms==0.1.9

from sklearn import linear_model
from sklearn.ensemble import RandomForestClassifier

X=pd.DataFrame(train_flatten_out)
print(X.shape)

X1 = pd.DataFrame(test_flatten_out)

y = []
for x in y_train:
  y.append(np.argmax(x))
y = np.array(y)

y1 = []
for x in y_test:
  y1.append(np.argmax(x))
y1 = np.array(y1)

from sklearn.model_selection import train_test_split

data_total = []
for x in train_flatten_out:
  data_total.append(x)
for x in test_flatten_out:
  data_total.append(x)  
data_total = np.array(data_total) 
print("Total size is ", data_total.shape)

target_total = []
for x in y:
  target_total.append(x)
for x in y1:
  target_total.append(x)  
target_total = np.array(target_total) 
print("Total Target size is ", target_total.shape)

X_train, X_test, y_train, y_test = train_test_split(data_total,target_total, test_size=0.20, random_state=42)

print('Train set shape is',X_train.shape)
print('Test set shape is',X_test.shape)
print("Target Train shape", y_train.shape)
print("Target Test shape", y_test.shape)

y1 = y_test
y = y_train
X = pd.DataFrame(X_train)
X1 = pd.DataFrame(X_test)

def f_per_particle(m, alpha):

  total_features = X.shape[1]
  
  if np.count_nonzero(m) == 0: 
        #if the particle subset is only zeros, get the original set of attributes
    X_subset = X
  else:
    X_subset = X.iloc[:,m==1]
  scores = cross_val_score(classifier, X_subset, y, cv=3)
  P = scores.mean()
  particleScore.append(P)
  particleSize.append(X_subset.shape[1])  
  j = (alpha * (1.0 - P)+ (1.0 - alpha) * (1 - (X_subset.shape[1] / total_features)))
  return j

def f(x, alpha=0.9):
  """Higher-level method to do classification in the
  whole swarm.

  Inputs
    ------
  x: numpy.ndarray of shape (n_particles, dimensions)
        The swarm that will perform the search

  Returns
  -------
  numpy.ndarray of shape (n_particles, )
        The computed loss for each particle
  """
  n_particles = x.shape[0]
  j = [f_per_particle(x[i], alpha) for i in range(n_particles)]
  #print("f j: ", j)
  return np.array(j)

from sklearn.ensemble import RandomForestRegressor

classifier = linear_model.LogisticRegression(max_iter=1000)

from sklearn.model_selection import cross_val_score
import matplotlib.pyplot as plt #MatPlotLib usado para desenhar o gráfico criado com o NetworkX

# Import PySwarms
import pyswarms as ps

# Commented out IPython magic to ensure Python compatibility.
# %load_ext autoreload
# %autoreload 2
# %matplotlib inline

from datetime import datetime as dt
import time
from pyswarms.utils.environments import PlotEnvironment
start = dt.now()
print("Started at: ", str(start))
particleScore = list()
particleSize = list()
#mySubsets = list()

# Initialize swarm, arbitrary
options = {'c1': 2, 'c2': 2, 'w':0.3, 'k': 20, 'p':2}

# Call instance of PSO
dimensions = X.shape[1] # dimensions should be the number of features
#optimizer.reset()
#optimizer = ps.single.GlobalBestPSO(n_particles=1, dimensions=dimensions,
#                                    options=options)
optimizer = ps.discrete.BinaryPSO(n_particles=20, dimensions=dimensions, options=options)

# Perform optimization
#cost, pos = optimizer.optimize(f, print_step=1, iters=10, verbose=2)


# Initialize plot environment
plt_env = PlotEnvironment(optimizer, f,10)

# Plot the cost
plt_env.plot_cost(figsize=(8,6));
plt.show()


#print(cost,pos)
end = dt.now()
print("Finished at: ", str(end))
total = end-start
print("Total time spent: ", total)

import matplotlib.pyplot as plt #MatPlotLib usado para desenhar o gráfico criado com o NetworkX

#iterations = list(range(1,len(optimizer.get_cost_history)+1))
plt.figure(figsize=(10,7))
#plt.xlabel('2^i classes')
plt.xlabel('subsetSize')
plt.ylabel('Accuracy')
plt.plot(particleSize, particleScore, 'bo')
#plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)
plt.grid(True)


#plt.savefig("D:/USP/2018-1/Computação Bioinspirada/Trabalhos/iterationVSerrorRate2.png", format="PNG")


plt.show()

X=pd.DataFrame(X)

from sklearn.model_selection import cross_val_score
# Create two instances of LogisticRegression
#classfier = linear_model.LogisticRegression()
classifier = RandomForestClassifier(bootstrap= False,max_depth= 10,
                                     max_features= 1,min_samples_leaf= 1,
                                     min_samples_split= 10,n_estimators= 12)

rank = list()

fullSet = cross_val_score(classifier, pd.DataFrame(X), y, cv=5)
print("Full set Accuracy: %0.2f (+/- %0.2f)" % (fullSet.mean(), fullSet.std() * 2))
print("----------------------------------------------------------------------------")
bests = optimizer.personal_best_pos #optimizer.get_pos_history
for b in bests:
    # Get the selected features from the final positions

    X_selected_features = X.iloc[:,b==1]  # subset

    # Perform classification and store performance in P
    classifier.fit(X_selected_features, y)
    scores = cross_val_score(classifier, X_selected_features, y, cv=5)
    print("Accuracy: %0.2f (+/- %0.2f)" % (scores.mean(), scores.std() * 2), b)
    rank.append([scores.mean(), b])
    # Compute performance
  
    subset_performance = (classifier.predict(X_selected_features) == y).mean()
    

    print('Subset performance: %.3f' % (subset_performance))

def individual_importance():
  sum = []
  i = 0
  total = 0 
  while i < bests.shape[0]:
    total += bests[i]
    i+=1
  print('Most Important Features:', total/bests.shape[0])

individual_importance()

import time
import numpy as np
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn import datasets
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC
import time
import numpy as np
from sklearn.ensemble import BaggingClassifier, RandomForestClassifier
from sklearn import datasets
from sklearn.multiclass import OneVsRestClassifier
from sklearn.svm import SVC

from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import VotingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.model_selection import GridSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB 
from sklearn.ensemble import RandomForestClassifier

import pandas as pd
import numpy as np
from scipy import interp

from  sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import LabelBinarizer

def class_report(y_true, y_pred, y_score=None, average='micro'):
  if y_true.shape != y_pred.shape:
    print("Error! y_true %s is not the same shape as y_pred %s" % (
          y_true.shape,
          y_pred.shape)
        )
    return

  lb = LabelBinarizer()

  if len(y_true.shape) == 1:
        lb.fit(y_true)

  #Value counts of predictions
  labels, cnt = np.unique(y_pred,return_counts=True)
  n_classes = len(labels)
  pred_cnt = pd.Series(cnt, index=labels)

  metrics_summary = precision_recall_fscore_support(y_true=y_true,y_pred=y_pred,labels=labels)

  avg = list(precision_recall_fscore_support(y_true=y_true, y_pred=y_pred,average='weighted'))

  metrics_sum_index = ['precision', 'recall', 'f1-score', 'support']
  class_report_df = pd.DataFrame(list(metrics_summary),index=metrics_sum_index,columns=labels)

  support = class_report_df.loc['support']
  total = support.sum() 
  class_report_df['avg / total'] = avg[:-1] + [total]

  class_report_df = class_report_df.T
  class_report_df['pred'] = pred_cnt
  class_report_df['pred'].iloc[-1] = total

  if not (y_score is None):
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for label_it, label in enumerate(labels):
      fpr[label], tpr[label], _ = roc_curve((y_true == label).astype(int), y_score[:, label_it])
      roc_auc[label] = auc(fpr[label], tpr[label])

    if average == 'micro':
      if n_classes <= 2:
        fpr["avg / total"], tpr["avg / total"], _ = roc_curve(lb.transform(y_true).ravel(), y_score[:, 1].ravel())
      else:
        fpr["avg / total"], tpr["avg / total"], _ = roc_curve(lb.transform(y_true).ravel(), y_score.ravel())

      roc_auc["avg / total"] = auc(fpr["avg / total"],tpr["avg / total"])

    elif average == 'macro':
      # First aggregate all false positive rates
      all_fpr = np.unique(np.concatenate([fpr[i] for i in labels]))
      # Then interpolate all ROC curves at this points
      mean_tpr = np.zeros_like(all_fpr)
      for i in labels:
        mean_tpr += interp(all_fpr, fpr[i], tpr[i])
        # Finally average it and compute AUC
      mean_tpr /= n_classes
      fpr["macro"] = all_fpr
      tpr["macro"] = mean_tpr
      roc_auc["avg / total"] = auc(fpr["macro"], tpr["macro"])

    class_report_df['AUC'] = pd.Series(roc_auc)

  return class_report_df

!pip install scikit-plot

import scikitplot as skplt
import matplotlib.pyplot as plt

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.ensemble import RandomForestRegressor

classifier = RandomForestClassifier(bootstrap= False,max_depth= 10,
                                     max_features= 1,min_samples_leaf= 1,
                                     min_samples_split= 10,n_estimators= 12)

X_selected_features = X.iloc[:,bests[-2]==1]
X_selected_features_test = X1.iloc[:,bests[-2]==1]
classifier.fit(X_selected_features, y)
probas =  classifier.predict_proba(X_selected_features_test)
y_predsm= classifier.predict(X_selected_features_test)
y_testsm = y1.reshape(y1.shape[0])
y_predsm = y_predsm.reshape(y1.shape[0])
print(y_testsm.shape,y_predsm.shape)
skplt.metrics.plot_precision_recall(y_testsm,probas)
skplt.metrics.plot_roc_curve(y_testsm,probas,title="ROC Curves with Ensemble Classifier", cmap='Blues', figsize=[7,7])
plt.show()
skplt.metrics.plot_confusion_matrix(y_testsm,y_predsm, normalize=True, cmap='Blues')
plt.show()
report_with_auc = class_report(
y_true=y_testsm, 
y_pred=classifier.predict(X_selected_features_test), 
y_score=classifier.predict_proba(X_selected_features_test))
print("Ensemble Classifier\n",report_with_auc)